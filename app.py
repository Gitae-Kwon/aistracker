# app.py
# -*- coding: utf-8 -*-
import os
from datetime import date
from dateutil.relativedelta import relativedelta
from urllib.parse import quote

import streamlit as st
import pandas as pd
import requests
from requests.adapters import HTTPAdapter, Retry
from pytrends.request import TrendReq

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="AI Tools Popularity Tracker", page_icon="ğŸ“ˆ", layout="wide")

DEFAULT_KEYWORDS = ["ChatGPT", "Midjourney", "GitHub Copilot", "Google Gemini"]
DEFAULT_REGION = "US"  # '', 'US', 'KR' ë“±
DEFAULT_START = date(2022, 1, 1)
DEFAULT_END = date.today()

# ìœ„í‚¤ ë¬¸ì„œ ë§¤í•‘(ì˜ì–´ ìœ„í‚¤ ê¸°ì¤€). í•„ìš”ì‹œ ì‚¬ì´ë“œë°”ì—ì„œ ìˆ˜ì • ê°€ëŠ¥
DEFAULT_WIKI_PAGES = {
    "ChatGPT": "ChatGPT",
    "Midjourney": "Midjourney",
    "GitHub Copilot": "GitHub_Copilot",
    "Google Gemini": "Google_Gemini",
}

# Wikimedia ìš”ì²­ ì„¸ì…˜ (User-Agent í•„ìˆ˜)
SESSION = requests.Session()
SESSION.headers.update({
    # ì—°ë½ì²˜/í”„ë¡œì íŠ¸ ë§í¬ í¬í•¨ ê¶Œì¥(ë³¸ì¸ ì´ë©”ì¼/ê¹ƒí—™ ì´ìŠˆ URL ë“±ìœ¼ë¡œ êµì²´í•˜ì„¸ìš”)
    "User-Agent": "AI-Tools-Tracker/1.0 (contact: your_email@example.com)"
})
retries = Retry(
    total=5,
    backoff_factor=1.5,
    status_forcelist=[403, 429, 500, 502, 503, 504],
    allowed_methods=["GET"]
)
SESSION.mount("https://", HTTPAdapter(max_retries=retries))

# =========================
# í•¨ìˆ˜: Google Trends
# =========================
def _build_pytrends():
    # tz: 360 = GMT+6ê°€ ì•„ë‹ˆë¼, pytrends ë‚´ë¶€ í‘œì¤€. í¬ê²Œ ì˜í–¥ ì—†ìŒ
    return TrendReq(hl="en-US", tz=360)

def _chunks(lst, n):
    """lstë¥¼ ê¸¸ì´ në¡œ ìª¼ê°œëŠ” ì œë„ˆë ˆì´í„°"""
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

@st.cache_data(ttl=60*60)  # 1ì‹œê°„ ìºì‹œ
def fetch_google_trends_monthly_mean(keywords, start: date, end: date, region: str = "") -> pd.DataFrame:
    """
    Google Trends ê´€ì‹¬ë„(0-100 ìƒëŒ€ì§€í‘œ) ì›” í‰ê· ìœ¼ë¡œ ì§‘ê³„í•´ì„œ ë°˜í™˜.
    - ìš”ì²­ì„ 4~5ê°œ í‚¤ì›Œë“œ ë‹¨ìœ„ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì•ˆì •ì„± í™•ë³´
    - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê³  ë‚˜ë¨¸ì§€ ë³‘í•©
    """
    if not keywords:
        return pd.DataFrame()

    time_window = f"{start.isoformat()} {end.isoformat()}"
    pytrends = _build_pytrends()
    frames = []
    for batch in _chunks(keywords, 5):
        try:
            pytrends.build_payload(batch, timeframe=time_window, geo=region)
            df = pytrends.interest_over_time()
            if df is None or df.empty:
                continue
            if 'isPartial' in df.columns:
                df = df.drop(columns=['isPartial'])
            # ì›” í‰ê· 
            df = df.resample("MS").mean().round(2)
            frames.append(df)
        except Exception as e:
            # pytrendsê°€ ê°€ë” ë§‰íˆëŠ” ê²½ìš°ê°€ ìˆìŒ (IP/ë¹ˆë„/ì„¸ì…˜ ì´ìŠˆ)
            st.warning(f"âš ï¸ Google Trends ìš”ì²­ ì¼ë¶€ ì‹¤íŒ¨: {batch} | {e}")
            continue

    if not frames:
        return pd.DataFrame()
    # ë™ì¼ ë‚ ì§œ ì¸ë±ìŠ¤ ê¸°ì¤€ ë³‘í•©
    base = frames[0]
    for f in frames[1:]:
        base = base.join(f, how="outer")
    base.index.name = "month"
    # ì¼ë¶€ í‚¤ì›Œë“œë§Œ ì„±ê³µí–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
    cols = [k for k in keywords if k in base.columns]
    return base[cols].sort_index()

# =========================
# í•¨ìˆ˜: Wikimedia Pageviews
# =========================
def _wiki_month_bounds(start: date, end: date):
    start_str = pd.Timestamp(start).strftime("%Y%m01")
    end_last = (pd.Timestamp(end) + relativedelta(day=31)).strftime("%Y%m%d")
    return start_str, end_last

def _wiki_url(project, access, agent, article, start_str, end_last):
    encoded_title = quote(article, safe="")
    return (
        f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/"
        f"{project}/{access}/{agent}/{encoded_title}/monthly/{start_str}/{end_last}"
    )

def fetch_wiki_pageviews_one(title: str, start: date, end: date,
                             project="en.wikipedia", access="all-access", agent="user") -> pd.Series:
    start_str, end_last = _wiki_month_bounds(start, end)
    url = _wiki_url(project, access, agent, title, start_str, end_last)
    try:
        r = SESSION.get(url, timeout=30)
        r.raise_for_status()
        items = r.json().get("items", [])
        if not items:
            return pd.Series(dtype="float64")

        data = {}
        for it in items:
            ts = str(it["timestamp"])  # e.g., 2025010100
            y, m = int(ts[:4]), int(ts[4:6])
            data[pd.Timestamp(year=y, month=m, day=1)] = it["views"]
        s = pd.Series(data).sort_index()
        return s
    except Exception as e:
        # 403/429 ë“± ë°œìƒ ì‹œ ë¹ˆ ì‹œë¦¬ì¦ˆ ë°˜í™˜(ì•±ì´ ì£½ì§€ ì•Šë„ë¡)
        st.info(f"â„¹ï¸ ìœ„í‚¤ ì¡°íšŒ ì‹¤íŒ¨: {title} | {e}")
        return pd.Series(dtype="float64")

@st.cache_data(ttl=60*60)
def fetch_wiki_pageviews_map(page_map: dict, start: date, end: date) -> pd.DataFrame:
    if not page_map:
        return pd.DataFrame()
    series = []
    for key, title in page_map.items():
        s = fetch_wiki_pageviews_one(title, start, end)
        s.name = key
        series.append(s)
    if not series:
        return pd.DataFrame()
    df = pd.concat(series, axis=1)
    df.index.name = "month"
    return df.sort_index()

# =========================
# UI
# =========================
st.title("ğŸ“ˆ AI ë„êµ¬ ì‹œì ë³„ ì¸ê¸°/ì¸ì§€ë„ íŠ¸ë˜ì»¤")
st.caption("Google Trends(ìƒëŒ€ì§€í‘œ) + Wikipedia Pageviews(ì ˆëŒ€ì¡°íšŒ)ë¥¼ ì›” ë‹¨ìœ„ë¡œ íŠ¸ë˜í‚¹í•©ë‹ˆë‹¤.")

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    region = st.selectbox("Google Trends ì§€ì—­ (ë¹ˆê°’=ê¸€ë¡œë²Œ)", options=["", "US", "KR", "BR", "JP", "GB", "DE", "FR"], index=1)
    start = st.date_input("ì‹œì‘ì¼", value=DEFAULT_START, min_value=date(2019,1,1), max_value=DEFAULT_END)
    end = st.date_input("ì¢…ë£Œì¼", value=DEFAULT_END, min_value=DEFAULT_START, max_value=DEFAULT_END)

    kw_text = st.text_area("í‚¤ì›Œë“œ(ì‰¼í‘œë¡œ êµ¬ë¶„)", value=", ".join(DEFAULT_KEYWORDS))
    keywords = [k.strip() for k in kw_text.split(",") if k.strip()]

    st.markdown("---")
    st.subheader("Wikipedia ë¬¸ì„œ ë§¤í•‘")
    st.caption("í•„ìš” ì‹œ ë¬¸ì„œ ì œëª©ì„ ìˆ˜ì •í•˜ì„¸ìš”. (ì˜ë¬¸ ìœ„í‚¤ ê¸°ì¤€)")
    wiki_map = {}
    for k in keywords:
        default_title = DEFAULT_WIKI_PAGES.get(k, k.replace(" ", "_"))
        wiki_map[k] = st.text_input(f"{k} â†’", value=default_title, key=f"wiki_{k}")

    use_wiki = st.checkbox("Wikipedia Pageviews ì‚¬ìš©", value=True)
    st.markdown("---")
    st.caption("Tip: 403/429ê°€ ëœ¨ë©´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ ì¤„ì—¬ë³´ì„¸ìš”.")

# =========================
# ë°ì´í„° ìˆ˜ì§‘ & í‘œì‹œ
# =========================
col1, col2 = st.columns(2)

with st.spinner("Google Trends ìˆ˜ì§‘ ì¤‘..."):
    trends_df = fetch_google_trends_monthly_mean(keywords, start, end, region)

if not trends_df.empty:
    with col1:
        st.subheader("Google Trends (ì›” í‰ê· , 0â€“100 ìƒëŒ€ì§€í‘œ)")
        st.line_chart(trends_df, height=340, use_container_width=True)
        st.dataframe(trends_df.tail(12), use_container_width=True)
        st.download_button(
            "â¬‡ï¸ Trends CSV ë‹¤ìš´ë¡œë“œ",
            trends_df.to_csv(index=True).encode("utf-8"),
            file_name=f"google_trends_{region or 'GLOBAL'}_{start}_{end}.csv",
            mime="text/csv"
        )
else:
    with col1:
        st.warning("Trends ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ/ê¸°ê°„/ì§€ì—­ì„ ì¡°ì •í•˜ê±°ë‚˜ ì ì‹œ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")

if use_wiki:
    with st.spinner("Wikipedia Pageviews ìˆ˜ì§‘ ì¤‘..."):
        wiki_df = fetch_wiki_pageviews_map(wiki_map, start, end)
else:
    wiki_df = pd.DataFrame()

if use_wiki:
    if not wiki_df.empty:
        with col2:
            st.subheader("Wikipedia Pageviews (ì›”ë³„ ì ˆëŒ€ ì¡°íšŒìˆ˜)")
            st.line_chart(wiki_df, height=340, use_container_width=True)
            st.dataframe(wiki_df.tail(12), use_container_width=True)
            st.download_button(
                "â¬‡ï¸ Wiki CSV ë‹¤ìš´ë¡œë“œ",
                wiki_df.to_csv(index=True).encode("utf-8"),
                file_name=f"wikipedia_pageviews_{start}_{end}.csv",
                mime="text/csv"
            )
    else:
        with col2:
            st.info("Wikipedia Pageviewsë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ì œëª©/ë„¤íŠ¸ì›Œí¬/User-Agentë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# =========================
# í†µí•© í…Œì´ë¸” (ì„ íƒ)
# =========================
if not trends_df.empty or (use_wiki and not wiki_df.empty):
    combined = trends_df.join(wiki_df, how="outer", lsuffix="_trends", rsuffix="_pageviews")
    st.markdown("### ğŸ”— í†µí•© ë·° (Trends + Wiki)")
    st.dataframe(combined.tail(12), use_container_width=True)
    st.download_button(
        "â¬‡ï¸ Combined CSV ë‹¤ìš´ë¡œë“œ",
        combined.to_csv(index=True).encode("utf-8"),
        file_name=f"ai_tools_combined_{start}_{end}.csv",
        mime="text/csv"
    )

# =========================
# í‘¸í„°: ì°¸ê³ /ì£¼ì˜
# =========================
with st.expander("â„¹ï¸ ì°¸ê³  ë° ì£¼ì˜ì‚¬í•­"):
    st.markdown("""
- **Google TrendsëŠ” ìƒëŒ€ì§€í‘œ(0â€“100)** ì…ë‹ˆë‹¤. ë™ì¼ ìš”ì²­(í‚¤ì›Œë“œÂ·ê¸°ê°„Â·ì§€ì—­ ì¡°í•©) ë‚´ì—ì„œì˜ ë¹„êµì— ì í•©í•˜ë©°, ë‹¤ë¥¸ ìš”ì²­ê³¼ëŠ” ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Wikipedia PageviewsëŠ” ì ˆëŒ€ ì¡°íšŒìˆ˜**ë¡œ ê´€ì‹¬ì˜ ê·œëª¨ê°ì„ ë³´ì™„í•©ë‹ˆë‹¤. í˜ì´ì§€ ì œëª© ë³€ê²½/ë¦¬ë‹¤ì´ë ‰íŠ¸ê°€ ìˆëŠ”ì§€ ê°€ë” í™•ì¸í•˜ì„¸ìš”.
- API í˜¸ì¶œì€ ë¹ˆë„ ì œí•œ/ì°¨ë‹¨(403/429)ì— ë¯¼ê°í•  ìˆ˜ ìˆìœ¼ë‹ˆ, **í‚¤ì›Œë“œë¥¼ ë‚˜ëˆ  ìš”ì²­**í•˜ê³  **ìºì‹œ**ë¥¼ í™œìš©í•˜ì„¸ìš”.
- User-Agentì—ëŠ” ì—°ë½ì²˜(ì´ë©”ì¼/ê¹ƒí—™ URL)ë¥¼ ëª…ì‹œí•˜ëŠ” ê²ƒì´ **Wikimedia ê¶Œê³ **ì…ë‹ˆë‹¤.
""")
